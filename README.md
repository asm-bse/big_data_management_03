
# Housing Incidents Prediction Pipeline with Airflow, Spark, and MLflow

This project builds an end-to-end machine learning pipeline for predicting housing-related incidents based on property features. The pipeline is orchestrated using Apache Airflow, powered by PySpark for data processing and MLflow for model tracking and deployment.

## ğŸš€ Project Overview

- **Data Source**: Real estate and income datasets stored in local Parquet format.
- **ML Task**: Binary classification â€” whether a house is in a high-price-per-room zone.
- **Models Used**: Logistic Regression and Decision Tree (Spark MLlib).
- **Orchestration**: Apache Airflow DAG (`ml_pipeline_dag.py`).
- **Model Logging & Deployment**: MLflow Tracking Server with local backend store.

## ğŸ“ Project Structure

```
.
â”œâ”€â”€ airflow/                   # Airflow DAGs, logs, plugins
â”‚   â””â”€â”€ dags/ml_pipeline_dag.py
â”œâ”€â”€ best_model/               # Exported model artifacts
â”œâ”€â”€ docker-compose.yml        # Docker services (Airflow, MLflow)
â”œâ”€â”€ Dockerfile.airflow        # Custom Dockerfile for Airflow service
â”œâ”€â”€ mlruns/                   # MLflow local run history
â”œâ”€â”€ src/
â”‚   â””â”€â”€ mlflow_pipeline.py    # Core training pipeline
â”œâ”€â”€ zones/                    # Raw, formatted, exploitation data zones
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ README.md                 # This file
```

## ğŸ› ï¸ How to Run the Project

### 1. Build and Start Docker Containers

Run the following commands to set up the services:

```bash
docker compose down -v        # Clean previous volumes and containers
docker compose up --build     # Build and launch MLflow + Airflow
```

This will spin up:

- An **MLflow Tracking Server** at http://localhost:5050
- An **Airflow Web UI** at http://localhost:8080

### 2. Access Airflow

Visit http://localhost:8080 and log in with:

- **Username**: `admin`
- **Password**: `admin`

Trigger the DAG named `mlflow_pipeline_dag`.

### 3. What the DAG Does

- Loads dataset from the exploitation zone (`zones/exploitation/idealista`)
- Trains two models with Spark MLlib
- Evaluates accuracy
- Logs metrics and models to MLflow
- Promotes the best model to Production stage

## âœ… Features

- End-to-end ML lifecycle automation
- Local MLflow server with Spark model logging
- Clear model comparison based on accuracy
- Model promotion to production-ready stage
- Dockerized for easy reproducibility

## ğŸ“¦ Dependencies

Dependencies are automatically installed inside the Docker container using the provided `Dockerfile.airflow` and `requirements.txt`.

## âœï¸ Authors

- **Aleksandr Smolin**
- **Maria Simakova**

## ğŸ“„ License

MIT License
