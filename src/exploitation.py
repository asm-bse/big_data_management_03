#!/usr/bin/env python
"""
exploitation.py – Data enrichment & feature extraction for Lab 3 (Data Management Backbone)
===============================================================================
Reads standardized datasets from the Formatted Zone, performs cleaning, aggregation, and feature engineering,
and writes prepared tables into the Exploitation Zone for downstream analysis. Also validates data quality.

Usage examples:
  # Preview exploitation output without writing
  uv run python -- exploitation.py \
      --formatted-base zones/formatted \
      --exploitation-base zones/exploitation_preview \
      --format parquet \
      --preview

  # Write exploitation data and overwrite
  uv run python -- exploitation.py \
      --formatted-base zones/formatted \
      --exploitation-base zones/exploitation \
      --format parquet --overwrite
"""
import argparse
import logging
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, year, count as _count, try_divide

# Configure minimal logging: show only warnings and errors
logging.basicConfig(level=logging.WARNING, format="%(levelname)-8s %(message)s")


def build_spark() -> SparkSession:
    spark = (
        SparkSession.builder
            .appName("BDM-Lab3-Exploitation")
            .config("spark.ui.showConsoleProgress", "false")
            .getOrCreate()
    )
    # suppress Spark INFO logs
    spark.sparkContext.setLogLevel("ERROR")
    return spark


def read_formatted(spark: SparkSession, formatted_base: str):
    # Load Parquet tables from Formatted Zone
    paths = {
        "idealista": f"{formatted_base}/idealista",
        "incidences": f"{formatted_base}/incidences",
        "income": f"{formatted_base}/income"
    }
    dfs = {
        name: spark.read.format("parquet").load(path)
        for name, path in paths.items()
    }
    return dfs


def transform_and_write(dfs: dict, exploitation_base: str, fmt: str, overwrite: bool):
    mode = "overwrite" if overwrite else "errorifexists"

    # 1. Clean Idealista: drop rows with missing price, size, or rooms, add price_per_room safely
    df_ideal = (
        dfs['idealista']
        .dropna(subset=['price', 'size', 'rooms'])
        .withColumn('price_per_room', try_divide(col('price'), col('rooms')))
    )
    df_ideal.write.format(fmt).mode(mode).save(f"{exploitation_base}/idealista")

    # 2. Aggregate Incidences: count incidents per district per year
    df_inc_agg = (
        dfs['incidences']
        .groupBy('CODI_DISTRICTE', 'year')
        .agg(_count('*').alias('incidence_count'))
    )
    df_inc_agg.write.format(fmt).mode(mode).save(f"{exploitation_base}/incidences")

    # 3. Prepare Income: select relevant columns and drop nulls
    df_income = (
        dfs['income']
        .dropna(subset=['avg_income'])
        .select('year', 'Codi_Barri', 'Nom_Barri', 'avg_income')
    )
    df_income.write.format(fmt).mode(mode).save(f"{exploitation_base}/income")

    logging.warning(f"Exploitation data written to {exploitation_base} (format={fmt})")


def validate(dfs: dict, name: str):
    df = dfs[name]
    # minimal validation output
    count = df.count()
    nulls = df.select(*[(df[c].isNull().cast('int')).alias(c) for c in df.columns])
    total_nulls = nulls.groupBy().sum().collect()[0].asDict()
    logging.warning(f"{name}: rows={count}, nulls={total_nulls}")


def main():
    parser = argparse.ArgumentParser(
        description="Prepare Exploitation Zone tables for analysis."
    )
    parser.add_argument("--formatted-base", "-f", required=True,
                        help="Base path for Formatted Zone Parquet data")
    parser.add_argument("--exploitation-base", "-e", required=True,
                        help="Target path for Exploitation Zone data")
    parser.add_argument("--format", choices=["parquet","csv","delta"],
                        default="parquet", help="Output data format")
    parser.add_argument("--overwrite", action="store_true",
                        help="Overwrite existing exploitation data")
    parser.add_argument("--preview", action="store_true",
                        help="Preview without writing")
    args = parser.parse_args()

    spark = build_spark()
    dfs = read_formatted(spark, args.formatted_base)

    if args.preview:
        # show only schemas and counts
        for name, df in dfs.items():
            print(f"=== {name} schema ===")
            df.printSchema()
            print(f"rows: {df.count()}")
    else:
        transform_and_write(dfs, args.exploitation_base, args.format, args.overwrite)
        dfs_exp = read_formatted(spark, args.exploitation_base)
        for name in dfs_exp:
            validate(dfs_exp, name)
    spark.stop()


if __name__ == "__main__":
    main()
