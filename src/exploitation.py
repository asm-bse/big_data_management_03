#!/usr/bin/env python
"""
exploitation.py – Data aggregation & feature creation for Lab 3
================================================================
This script reads standardized datasets from the Formatted Zone, performs the final
stage of transformations, and writes analysis-ready tables into the Exploitation Zone.

The main steps are:
1.  Reads the cleaned 'idealista', 'incidences', and 'income' tables.
2.  Filters out records that couldn't be matched to a district_id.
3.  Aggregates data to create meaningful KPIs at the district-year level.
4.  **Handles Missing Yearly Data**: Uses Window functions to fill gaps in property
    data by propagating the district-level average to years without idealista data.
5.  Joins all KPIs into a single, analysis-ready "master table".
6.  Writes the final aggregated table to the Exploitation Zone.
"""
import argparse
import logging
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, count
from pyspark.sql.window import Window

logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")

def build_spark() -> SparkSession:
    return SparkSession.builder.appName("BDM-Lab3-Exploitation").getOrCreate()

def read_formatted(spark: SparkSession, formatted_base: str) -> dict:
    logging.info("Reading data from Formatted Zone: %s", formatted_base)
    paths = {
        "idealista": f"{formatted_base}/idealista",
        "incidences": f"{formatted_base}/incidences",
        "income": f"{formatted_base}/income"
    }
    dfs = {name: spark.read.parquet(path) for name, path in paths.items()}
    return dfs

def transform_and_write(dfs: dict, exploitation_base: str, fmt: str, overwrite: bool):
    mode = "overwrite" if overwrite else "errorifexists"

    idealista_df = dfs['idealista'].filter(col('district_id').isNotNull())
    incidences_df = dfs['incidences'].filter(col('district_id').isNotNull())
    income_df = dfs['income'].filter(col('district_id').isNotNull())
    
    # --- KPI Aggregation ---
    idealista_kpis_yearly = idealista_df.groupBy("district_id", "year").agg(
        avg("price").alias("avg_price"),
        avg("size").alias("avg_size"),
        avg("priceByArea").alias("avg_price_per_sqm"),
        count("*").alias("property_count")
    )

    incidences_kpis = incidences_df.groupBy("district_id", "year").agg(
        count("*").alias("incidence_count")
    )

    income_kpis = income_df.withColumnRenamed("Índex RFD Barcelona = 100", "avg_income_index") \
        .groupBy("district_id", "year").agg(
            avg(col("avg_income_index").cast("double")).alias("avg_income_index")
        )
    
    # --- Create Master Table by Joining all yearly KPIs ---
    # First, create a base table with all unique district-year combinations
    all_years_df = incidences_kpis.select("district_id", "year").union(income_kpis.select("district_id", "year")).distinct()

    # Calculate district-wide averages for property stats to fill gaps
    district_wide_idealista_kpis = idealista_df.groupBy("district_id").agg(
        avg("price").alias("district_avg_price"),
        avg("size").alias("district_avg_size"),
        avg("priceByArea").alias("district_avg_price_per_sqm"),
        count("*").alias("district_property_count")
    )

    # Join yearly data, then fill gaps in property data with the district-wide average
    master_df = all_years_df \
        .join(idealista_kpis_yearly, on=["district_id", "year"], how="left") \
        .join(incidences_kpis, on=["district_id", "year"], how="left") \
        .join(income_kpis, on=["district_id", "year"], how="left") \
        .join(district_wide_idealista_kpis, on="district_id", how="left")

    # Use coalesce to fill nulls in yearly property stats with the district-wide average
    from pyspark.sql.functions import coalesce
    master_df = master_df.withColumn("avg_price", coalesce(col("avg_price"), col("district_avg_price"))) \
        .withColumn("avg_size", coalesce(col("avg_size"), col("district_avg_size"))) \
        .withColumn("avg_price_per_sqm", coalesce(col("avg_price_per_sqm"), col("district_avg_price_per_sqm"))) \
        .withColumn("property_count", coalesce(col("property_count"), col("district_property_count"))) \
        .drop("district_avg_price", "district_avg_size", "district_avg_price_per_sqm", "district_property_count")

    master_df = master_df.orderBy("district_id", "year")

    output_path = f"{exploitation_base}/district_kpis"
    logging.info(f"Writing master table to {output_path}")
    master_df.write.format(fmt).mode(mode).save(output_path)

def main():
    parser = argparse.ArgumentParser(description="Prepare Exploitation Zone tables.")
    parser.add_argument("--formatted-base", "-f", required=True)
    parser.add_argument("--exploitation-base", "-e", required=True)
    parser.add_argument("--format", default="parquet")
    parser.add_argument("--overwrite", action="store_true")
    args = parser.parse_args()

    spark = build_spark()
    spark.sparkContext.setLogLevel("ERROR")
    dfs = read_formatted(spark, args.formatted_base)
    transform_and_write(dfs, args.exploitation_base, args.format, args.overwrite)
    spark.stop()

if __name__ == "__main__":
    main()